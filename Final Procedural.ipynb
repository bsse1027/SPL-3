{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39121c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import ast\n",
    "import scipy.stats as stats\n",
    "import stanza\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d968d93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('./labelledOut.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b936b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.pop(\"isAnomaly\")\n",
    "test.pop(\"reason\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_countries = [\"Algeria\", \"Angola\", \"Argentina\", \"Australia\", \"Austria\", \"Bahrain\", \"Bangladesh\", \"Belarus\", \"Belgium\", \"Brazil\", \"Cambodia\", \"Cameroon\", \"Canada\" , \"Chile\", \"Colombia\", \"Costa Rica\", \"Czechia\", \"Denmark\", \"Dominican Republic\" , \"Ecuador\",\"Egypt\", \"El Salvador\",  \"Ethiopia\", \"Finland\",\"France\", \"Georgia\",\"Germany\",\"Ghana\",\"Greece\", \"Guatemala\",\"Hong Kong\",\"Hungary\", \"India\",\"Indonesia\",\"Ireland\",\"Israel\",\"Italy\",\"Japan\",\"Jordan\",\"Kazakhstan\",\"Kenya\",\"Kuwait\",\"Lebanon\",\"Madagascar\",\"Malaysia\",\"Mauritius\",\"Mexico\",\"Morocco\", \"Mozambique\", \"Myanmar (Burma)\",\"Nepal\",\"Netherlands\",\"New Zealand\",\"Nicaragua\",\"Nigeria\",\"Norway\",\"Oman\",\"Pakistan\",\"Panama\",\"Paraguay\",\"Peru\",\"Philippines\",\"Poland\",\"Portugal\",\"Puerto Rico\",\"Romania\",\"Russia\",\"Saudi Arabia\",\"Senegal\",\"Singapore\",\"Slovakia\",\"South Africa\",\"South Korea\",\"Spain\",\"Sri Lanka\",\"Sweden\",\"Switzerland\",\"Taiwan\",\"Tanzania\",\"Thailand\",\"Tunisia\",\"Turkey\",\"Uganda\",\"Ukraine\",\"United Arab Emirates\",\"United Kingdom\",\"United States\",\"Uruguay\",\"Uzbekistan\",\"Venezuela\",\"Vietnam\",\"Zambia\",\"Zimbabwe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc20865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_df(csv_path, nrows = None):\n",
    "#     json_cols = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "#     df = pd.read_csv(csv_path,\n",
    "#                      #converters are dict of functions for converting values in certain columns. Keys can either be integers or column labels.\n",
    "#                      #json.loads() method can be used to parse a valid JSON string and convert it into a Python Dictionary.\n",
    "#                      #It is mainly used for deserializing native string, byte, or byte array which consists of JSON data into Python Dictionary.\n",
    "#                      converters = {col: json.loads for col in json_cols},                                                                         \n",
    "#                          dtype = {'fullVisitorId': 'str'}, # Important!!\n",
    "#                          nrows = nrows)\n",
    "#     for col in json_cols:\n",
    "#         # for each column, flatten data frame such that the values of a single col are spread in different cols\n",
    "#         # This will use subcol as names of flat_col.columns\n",
    "#         flat_col = json_normalize(df[col])\n",
    "#         # Name the columns in this flatten data frame as col.subcol for tracability\n",
    "#         flat_col.columns = [f\"{col}.{subcol}\" for subcol in flat_col.columns]\n",
    "#         # Drop the json_col and instead add the new flat_col\n",
    "#         df = df.drop(col, axis = 1).merge(flat_col, right_index = True, left_index = True)\n",
    "#     return df\n",
    "\n",
    "\n",
    "# csv_test_path = './test_v2.csv'\n",
    "# test = load_df(csv_test_path, nrows = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb8fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d79c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify non opearting countries \n",
    "lowCountries=[]\n",
    "hitWithCountry = {}\n",
    "sum = 0\n",
    "cnt = test['geoNetwork.country'].value_counts(sort=True)\n",
    "print(cnt)\n",
    "for i in range(0, cnt.size):\n",
    "    sum = sum + cnt[i]\n",
    "\n",
    "#Identify the Non Operating Countries\n",
    "\n",
    "for i in range(0, cnt.size):\n",
    "    #print(f\"Country Name: {cnt.index[i]} Frequency: {(cnt[i]/sum)*100}%\" )\n",
    "    if((cnt[i]/sum)*100 < 0.01):\n",
    "        lowCountries.append(cnt.index[i]) \n",
    "len(lowCountries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0328bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration for Rule 1\n",
    "#categories = df['Predictions'].to_numpy()\n",
    "colormap = np.array(['g', 'r'])\n",
    "hits1 = {}\n",
    "test['totals.hits'] = pd.to_numeric(test['totals.hits'])\n",
    "for indx in range(test.index.start, test.index.stop):\n",
    "    #print(indx)\n",
    "    country = test.at[indx, 'geoNetwork.country']\n",
    "    oneIndex = (test.at[indx,'hits'])\n",
    "    #hitsInfo = ast.literal_eval(oneIndex)\n",
    "    #print(hitsInfo)\n",
    "    if country in lowCountries:\n",
    "        if country not in hits1.keys():\n",
    "            hits1[country] = (test.at[indx, 'totals.hits'])\n",
    "        elif country in hits1.keys():\n",
    "            hits1[country] = hits1[country] + (test.at[indx, 'totals.hits'])\n",
    "len(hits1)\n",
    "f = plt.figure(figsize=(12, 4))\n",
    "f = plt.scatter(hits1.keys(), hits1.values())\n",
    "f = plt.xlabel('country')\n",
    "f = plt.ylabel('Total Hits')\n",
    "f = plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08004356",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##First Rule : If total hits of a session from NON Opearting/rare countries is outlier means 1.5 IQR from Q3\n",
    "cnt=0\n",
    "sum=0\n",
    "lowCountries=[]\n",
    "hitWithCountry = {}\n",
    "cnt = test['geoNetwork.country'].value_counts(sort=True)\n",
    "\n",
    "for i in range(0, cnt.size):\n",
    "    sum = sum + cnt[i]\n",
    "\n",
    "#Identify the Non Operating Countries\n",
    "\n",
    "for i in range(0, cnt.size):\n",
    "    #print(f\"Country Name: {cnt.index[i]} Frequency: {(cnt[i]/sum)*100}%\" )\n",
    "    if((cnt[i]/sum)*100 < 0.01):\n",
    "        lowCountries.append(cnt.index[i]) \n",
    "    \n",
    "\n",
    "inputCountries = test.at[0,'geoNetwork.country']\n",
    "\n",
    "test['totals.hits'] = pd.to_numeric(test['totals.hits'])\n",
    "data = test['totals.hits']\n",
    "np.median(data)\n",
    "upper_q = np.percentile(data, 75)\n",
    "lower_q = np.percentile(data, 25)\n",
    "iqr = upper_q - lower_q\n",
    "upper_whisker = data[data<=upper_q+1.5*iqr].max()\n",
    "lower_whisker = data[data>=lower_q-1.5*iqr].min()\n",
    "#print(upper_whisker)\n",
    "\n",
    "for indx in range(test.index.start, test.index.stop):\n",
    "    #print(indx)\n",
    "    country = test.at[indx, 'geoNetwork.country']\n",
    "    oneIndex = (test.at[indx,'hits'])\n",
    "    #hitsInfo = ast.literal_eval(oneIndex)\n",
    "    #print(hitsInfo)\n",
    "    if country not in op_countries:\n",
    "        hit = test.at[indx, 'totals.hits']\n",
    "        if int(hit) > upper_whisker:\n",
    "#             print(indx)\n",
    "            test.loc[indx, 'isAnomaly'] = 1\n",
    "            test.loc[indx, 'reason'] = \"total hits of a session from NON Opearting/rare countries is outlier\"\n",
    "#            print(f\"Index Number : {indx} Anomalous traffic Info: {country} Total Hits : {int(hit)} Visit Date:{test.at[indx, 'date']}\")\n",
    "#             for ind in range(0, len(hitsInfo)):\n",
    "#                 print(f\"Hit Number: {hitsInfo[ind]['hitNumber']} Visited Page: {hitsInfo[ind]['appInfo']['screenName']} Keyword: {test.at[indx, 'trafficSource.keyword']}\")\n",
    "        #print(f\"Country Name: {country} --> Total hits: {hit}\")\n",
    "#         if country in hitWithCountry.keys():\n",
    "#             hitWithCountry[country] += 1\n",
    "#         else:\n",
    "#             hitWithCountry[country]=1\n",
    "\n",
    "#newArr = test['hits'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c3fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "oneIndex = (test.at[0,'hits'])\n",
    "result = ast.literal_eval(oneIndex)\n",
    "#print(type(result))\n",
    "for ind in range(0, len(result)):\n",
    "#     temp = ast.literal_eval(oneIndex)\n",
    "#     print(f\"{temp}\")\n",
    "    print(f\"Hit Number: {result[ind]['page']['pageTitle']} Visited Page: {result[ind]['appInfo']['screenName']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2529dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration for Rule 2\n",
    "#categories = df['Predictions'].to_numpy()\n",
    "colormap = np.array(['g', 'r'])\n",
    "views1 = {}\n",
    "test['totals.pageviews'] = pd.to_numeric(test['totals.pageviews'])\n",
    "for indx in range(test.index.start, test.index.stop):\n",
    "    #print(indx)\n",
    "    country = test.at[indx, 'geoNetwork.country']\n",
    "    #oneIndex = (test.at[indx,'totals.pageviews'])\n",
    "    #hitsInfo = ast.literal_eval(oneIndex)\n",
    "    #print(hitsInfo)\n",
    "    if country in lowCountries:\n",
    "        if country not in views1.keys():\n",
    "            views1[country] = (test.at[indx, 'totals.pageviews'])\n",
    "        elif country in views1.keys():\n",
    "            views1[country] = views1[country] + (test.at[indx, 'totals.hits'])\n",
    "len(views1)\n",
    "f = plt.figure(figsize=(12, 4))\n",
    "f = plt.scatter(views1.keys(), views1.values())\n",
    "f = plt.xlabel('country')\n",
    "f = plt.ylabel('Total PageViews')\n",
    "f = plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Rule 2 : if total pageViews from a session is outlier from RareCountries\n",
    "sum = 0\n",
    "test['totals.pageviews'] = pd.to_numeric(test['totals.pageviews'])\n",
    "##Replace NaN Values with Zero\n",
    "test['totals.pageviews'].fillna(value = 0, inplace = True)\n",
    "data = test['totals.pageviews']\n",
    "upper_q = np.percentile(data, 75)\n",
    "lower_q = np.percentile(data, 25)\n",
    "iqr = upper_q - lower_q\n",
    "upper_whisker = data[data<=upper_q+1.5*iqr].max()\n",
    "lower_whisker = data[data>=lower_q-1.5*iqr].min()\n",
    "lower_whisker\n",
    "\n",
    "## Rule based on the Total PageView\n",
    "for ind in range(len(test)):\n",
    "    pageView = test.at[ind , 'totals.pageviews']\n",
    "    country = test.at[ind , 'geoNetwork.country']\n",
    "    if pageView > upper_whisker and country not in op_countries :\n",
    "        print(ind)\n",
    "        test.loc[ind, 'isAnomaly'] = 1\n",
    "        test.loc[ind, 'reason'] = \"total pageViews from a session is outlier from RareCountries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration for Rule 3\n",
    "#categories = df['Predictions'].to_numpy()\n",
    "colormap = np.array(['g', 'r'])\n",
    "\n",
    "f = plt.figure(figsize=(12, 4))\n",
    "f = plt.scatter(test['geoNetwork.country'], test['totals.pageviews'])\n",
    "f = plt.xlabel('country')\n",
    "f = plt.ylabel('Total PageViews')\n",
    "f = plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b24a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Rule 3 : if total pageViews from a session is absurdly higher from any Country\n",
    "sum = 0\n",
    "\n",
    "data = test['totals.pageviews']\n",
    "upper_q = np.percentile(data, 75)\n",
    "lower_q = np.percentile(data, 25)\n",
    "iqr = upper_q - lower_q\n",
    "upper_whisker = data[data<=upper_q+1.5*iqr].max()\n",
    "lower_whisker = data[data>=lower_q-1.5*iqr].min()\n",
    "lower_whisker\n",
    "\n",
    "## Rule based on the Total PageView\n",
    "for i in range(len(test)):\n",
    "    pageView = test.loc[i , 'totals.pageviews']\n",
    "    country = test.loc[i , 'geoNetwork.country']\n",
    "    if pageView > upper_whisker*10:\n",
    "        print(i)\n",
    "        test.loc[i, 'isAnomaly'] = 1\n",
    "        test.loc[i, 'reason'] = \"total pageViews from a session is absurdly higher from any Country\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c57637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test['totals.pageviews']\n",
    "plt.hist(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75173720",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip(test.fullVisitorId.duplicated().index, test.fullVisitorId.duplicated()):\n",
    "    if j == True:\n",
    "        print(test.loc[i, 'fullVisitorId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "test['totals.hits'] = pd.to_numeric(test['totals.hits'])\n",
    "mean = test['totals.hits'].mean()\n",
    "std = test['totals.hits'].std()\n",
    "maximum = test['totals.hits'].max()\n",
    "\n",
    "#plt.boxplot(test['totals.hits'])\n",
    "#plt.xlim(xmin=0, xmax = 501)\n",
    "#plt.show()\n",
    "data = test['totals.hits']\n",
    "np.median(data)\n",
    "upper_q = np.percentile(data, 75)\n",
    "lower_q = np.percentile(data, 25)\n",
    "iqr = upper_q - lower_q\n",
    "upper_whisker = data[data<=upper_q+1.5*iqr].max()\n",
    "lower_whisker = data[data>=lower_q-1.5*iqr].min()\n",
    "upper_whisker\n",
    "data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration for Rule 4\n",
    "#categories = df['Predictions'].to_numpy()\n",
    "colormap = np.array(['g', 'r'])\n",
    "tos1 = {}\n",
    "test['totals.timeOnSite'] = pd.to_numeric(test['totals.timeOnSite'])\n",
    "for indx in range(test.index.start, test.index.stop):\n",
    "    #print(indx)\n",
    "    country = test.at[indx, 'geoNetwork.country']\n",
    "    #oneIndex = (test.at[indx,'totals.pageviews'])\n",
    "    #hitsInfo = ast.literal_eval(oneIndex)\n",
    "    #print(hitsInfo)\n",
    "    if country in lowCountries:\n",
    "        if country not in tos1.keys():\n",
    "            tos1[country] = (test.at[indx, 'totals.pageviews'])\n",
    "        elif country in tos1.keys():\n",
    "            tos1[country] = tos1[country] + (test.at[indx, 'totals.hits'])\n",
    "len(tos1)\n",
    "f = plt.figure(figsize=(12, 4))\n",
    "f = plt.scatter(tos1.keys(), tos1.values())\n",
    "f = plt.xlabel('country')\n",
    "f = plt.ylabel('Session Duration')\n",
    "f = plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule 4: Session duration is outlier for non operating countries\n",
    "## total.timeOnSite means session duration of a session\n",
    "sum = 0\n",
    "test['totals.timeOnSite'] = pd.to_numeric(test['totals.timeOnSite'])\n",
    "test['totals.timeOnSite'].fillna(value = 0, inplace = True)\n",
    "data = test['totals.timeOnSite']\n",
    "upper_q = np.percentile(data, 75)\n",
    "lower_q = np.percentile(data, 25)\n",
    "iqr = upper_q - lower_q\n",
    "upper_whisker = data[data<=upper_q+1.5*iqr].max()\n",
    "lower_whisker = data[data>=lower_q-1.5*iqr].min()\n",
    "for i in range(len(test)):\n",
    "    timeOnSite = test.at[i, 'totals.timeOnSite']\n",
    "    country = test.at[i, 'geoNetwork.country']\n",
    "    if timeOnSite > upper_whisker and country in lowCountries:\n",
    "        print(i)\n",
    "        test.loc[i, 'isAnomaly'] = 1\n",
    "        test.loc[i, 'reason'] = \"Session duration is outlier for non operating countries\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c224906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining how many users come to our system in an hour to create a baseline for rule 5\n",
    "#test['visitStartTime'] = pd.to_datetime(test['visitStartTime'], unit='s')\n",
    "usersWithDate = {}\n",
    "test['date'] = pd.to_numeric(test['date'])\n",
    "test = test.sort_values(by = 'visitStartTime')\n",
    "test = test.reset_index(drop = True)\n",
    "#print(test)\n",
    "timeDiff = test['visitStartTime'].max() - test['visitStartTime'].min()\n",
    "dayDiff = timeDiff / 86400\n",
    "dayDiff = dayDiff.round()\n",
    "print(dayDiff)\n",
    "UsersPerDay = (test['fullVisitorId'].count() / dayDiff).round()\n",
    "UsersPerHour = (UsersPerDay/24).round()\n",
    "hourCount = 86400/24\n",
    "startTime = test.loc[0 , 'visitStartTime']\n",
    "sumOfUsers = 0\n",
    "userCounts=[]\n",
    "for itr in range(len(test)):\n",
    "    #print(test.loc[itr, 'visitStartTime'])\n",
    "    if(test.loc[itr, 'visitStartTime'] > (startTime + 3600)):\n",
    "        startTime = test.loc[itr, 'visitStartTime']\n",
    "        usersWithDate[startTime] = sumOfUsers\n",
    "        userCounts.append(sumOfUsers)\n",
    "        sumOfUsers = 0\n",
    "    sumOfUsers = sumOfUsers+1\n",
    "len(userCounts)\n",
    "#test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f8d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(userCounts)\n",
    "plt.boxplot(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ebaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories = df['Predictions'].to_numpy()\n",
    "colormap = np.array(['g', 'r'])\n",
    "\n",
    "f = plt.figure(figsize=(12, 4))\n",
    "f = plt.scatter(usersWithDate.keys(), usersWithDate.values())\n",
    "f = plt.xlabel('date')\n",
    "f = plt.ylabel('usersHour')\n",
    "f = plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d9f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule 5 : Detect Anomalous users generating higher volume of traffic in an hour\n",
    "sum = 0\n",
    "anomalousHours = []\n",
    "times = {}\n",
    "data = list(usersWithDate.values())\n",
    "upper_q = np.percentile(data, 75)\n",
    "lower_q = np.percentile(data, 25)\n",
    "iqr = upper_q - lower_q\n",
    "upper_whisker = upper_q+1.5*iqr\n",
    "lower_whisker = lower_q-1.5*iqr\n",
    "for i,j in usersWithDate.items():\n",
    "    if j > upper_whisker:\n",
    "        anomalousHours.append(i)\n",
    "\n",
    "for i in range(len(anomalousHours)):\n",
    "    for j in range(len(test)):\n",
    "        endTime = anomalousHours[i]\n",
    "        startTime = endTime - 3600\n",
    "        if test.loc[j, 'visitStartTime'] >=startTime and test.loc[j, 'visitStartTime'] <= endTime:\n",
    "            if j not in times:\n",
    "                times[j] = (test.loc[j, 'totals.hits'])\n",
    "            elif j not in times:\n",
    "                times[j] = times[j] + (test.loc[j, 'totals.hits'])\n",
    "    data = list(times.values())\n",
    "    upper_q = np.percentile(data, 75)\n",
    "    lower_q = np.percentile(data, 25)\n",
    "    iqr = upper_q - lower_q\n",
    "    upper_whisker = upper_q+1.5*iqr\n",
    "    lower_whisker = lower_q-1.5*iqr\n",
    "#     print(upper_whisker)\n",
    "    for a, b in times.items():\n",
    "        if b > upper_whisker*2:\n",
    "            print(a)\n",
    "            test.loc[a, 'isAnomaly'] = 1\n",
    "            test.loc[a, 'reason'] = \"user generating unusual higher volume of traffic in an hour\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule 6: Search Keywords having quotations or absurd file types\n",
    "new = test[test['trafficSource.keyword'].notna()]\n",
    "new.head()\n",
    "new = new[new['trafficSource.keyword'] != '(not set)']\n",
    "new = new[new['trafficSource.keyword'] != '(not provided)']\n",
    "new = new[new['trafficSource.keyword'] != '(automatic matching)']\n",
    "for index in new.index:\n",
    "    if \"\\\"\" in new.loc[index,'trafficSource.keyword'] or \".html\" in new.loc[index,'trafficSource.keyword'] or \"script\" in new.loc[index,'trafficSource.keyword']:\n",
    "        print(index)\n",
    "        test.loc[index, 'isAnomaly'] = 1\n",
    "        test.loc[index, 'reason'] = \"Search Keywords having SQL Injection, XSS Script or file extension types\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16221a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_countries = [\"Algeria\", \"Angola\", \"Argentina\", \"Australia\", \"Austria\", \"Bahrain\", \"Bangladesh\", \"Belarus\", \"Belgium\", \"Brazil\", \"Cambodia\", \"Cameroon\", \"Canada\" , \"Chile\", \"Colombia\", \"Costa Rica\", \"Czechia\", \"Denmark\", \"Dominican Republic\" , \"Ecuador\",\"Egypt\", \"El Salvador\",  \"Ethiopia\", \"Finland\",\"France\", \"Georgia\",\"Germany\",\"Ghana\",\"Greece\", \"Guatemala\",\"Hong Kong\",\"Hungary\", \"India\",\"Indonesia\",\"Ireland\",\"Israel\",\"Italy\",\"Japan\",\"Jordan\",\"Kazakhstan\",\"Kenya\",\"Kuwait\",\"Lebanon\",\"Madagascar\",\"Malaysia\",\"Mauritius\",\"Mexico\",\"Morocco\", \"Mozambique\", \"Myanmar (Burma)\",\"Nepal\",\"Netherlands\",\"New Zealand\",\"Nicaragua\",\"Nigeria\",\"Norway\",\"Oman\",\"Pakistan\",\"Panama\",\"Paraguay\",\"Peru\",\"Philippines\",\"Poland\",\"Portugal\",\"Puerto Rico\",\"Romania\",\"Russia\",\"Saudi Arabia\",\"Senegal\",\"Singapore\",\"Slovakia\",\"South Africa\",\"South Korea\",\"Spain\",\"Sri Lanka\",\"Sweden\",\"Switzerland\",\"Taiwan\",\"Tanzania\",\"Thailand\",\"Tunisia\",\"Turkey\",\"Uganda\",\"Ukraine\",\"United Arab Emirates\",\"United Kingdom\",\"United States\",\"Uruguay\",\"Uzbekistan\",\"Venezuela\",\"Vietnam\",\"Zambia\",\"Zimbabwe\"]\n",
    "\n",
    "op_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule 7: Sentiment Analysis on the searched keywords\n",
    "for index in new.index:\n",
    "    country = new.loc[index, 'geoNetwork.country']\n",
    "    if country not in op_countries:\n",
    "        doc = nlp(new.loc[index,'trafficSource.keyword'])\n",
    "        for i, sentence in enumerate(doc.sentences):\n",
    "            if sentence.sentiment == 0:\n",
    "                print(index)\n",
    "                test.loc[index, 'isAnomaly'] = 1\n",
    "                test.loc[index, 'reason'] = \"Sensitive Product Search or Service Lines\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c89d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rule 8 Attempt to visit employee only resource pages or newsletter registration pages\n",
    "sum = 0;\n",
    "for index in test.index:\n",
    "    country = test.loc[index, 'geoNetwork.country']\n",
    "    if country not in op_countries:\n",
    "        oneIndex = (test.loc[index,'hits'])\n",
    "        result = ast.literal_eval(oneIndex)\n",
    "        #print(type(result))\n",
    "        for ind in range(0, len(result)):\n",
    "            pageString = result[ind]['page']['pagePath']\n",
    "            if \"customerinfo\" in pageString:\n",
    "                print(ind)\n",
    "                test.loc[index, 'isAnomaly'] = 1\n",
    "                test.loc[index, 'reason'] = \"Attempt to visit employee only resource pages or newsletter registration pages from Non Operating Country\"\n",
    "#             sum = sum + 1\n",
    "        #     temp = ast.literal_eval(oneIndex)\n",
    "        #     print(f\"{temp}\")\n",
    "#             print(f\"Hit Number: {result[ind]['hitNumber']} Visited Page: {result[ind]['page']['pagePath']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb18ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index in test.index:\n",
    "#     print(index)\n",
    "#     if pd.isna(test.loc[index,'trafficSource.keyword']):\n",
    "#         test.loc[index, 'NLPValue'] = 1\n",
    "#     else :\n",
    "#         if (test.loc[index,'trafficSource.keyword'] == '(not set)'):\n",
    "#             test.loc[index, 'NLPValue'] = 1\n",
    "#         elif (test.loc[index,'trafficSource.keyword'] == '(not provided)'):\n",
    "#             test.loc[index, 'NLPValue'] = 1\n",
    "#         elif (test.loc[index,'trafficSource.keyword'] == '(automatic matching)'):\n",
    "#             test.loc[index, 'NLPValue'] = 1\n",
    "#         else:\n",
    "#             doc = nlp(test.loc[index,'trafficSource.keyword'])\n",
    "#             for i, sentence in enumerate(doc.sentences):\n",
    "#                 test.loc[index, 'NLPValue'] = sentence.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('./labelledOut.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36176b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['isAnomaly'].fillna(value = 0, inplace = True)\n",
    "aomalousSessions = test[test['isAnomaly'] == 1]\n",
    "len(aomalousSessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361377dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "aomalousSessions['reason']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a4e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "aomalousSessions.to_csv('./out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f9d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_for_ML = test[['date','device.deviceCategory','geoNetwork.country', 'totals.hits' , 'totals.pageviews', 'totals.timeOnSite', 'trafficSource.keyword', 'hits', 'NLPValue','isAnomaly', 'reason']]\n",
    "final_for_ML.to_csv('./finalwithDate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = final_for_ML\n",
    "dummy['hits'] = pd.factorize(dummy['hits'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f395b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
