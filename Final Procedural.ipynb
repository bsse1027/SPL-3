{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "39121c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import ast\n",
    "import scipy.stats as stats\n",
    "import stanza\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9d968d93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [204]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./labelledOut.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1255\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1253\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1255\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\parsers.pyx:883\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\parsers.pyx:1026\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\parsers.pyx:1072\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\parsers.pyx:1222\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\parsers.pyx:1235\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\parsers.pyx:1431\u001b[0m, in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('./labelledOut.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b936b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.pop(\"isAnomaly\")\n",
    "test.pop(\"reason\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_countries = [\"Algeria\", \"Angola\", \"Argentina\", \"Australia\", \"Austria\", \"Bahrain\", \"Bangladesh\", \"Belarus\", \"Belgium\", \"Brazil\", \"Cambodia\", \"Cameroon\", \"Canada\" , \"Chile\", \"Colombia\", \"Costa Rica\", \"Czechia\", \"Denmark\", \"Dominican Republic\" , \"Ecuador\",\"Egypt\", \"El Salvador\",  \"Ethiopia\", \"Finland\",\"France\", \"Georgia\",\"Germany\",\"Ghana\",\"Greece\", \"Guatemala\",\"Hong Kong\",\"Hungary\", \"India\",\"Indonesia\",\"Ireland\",\"Israel\",\"Italy\",\"Japan\",\"Jordan\",\"Kazakhstan\",\"Kenya\",\"Kuwait\",\"Lebanon\",\"Madagascar\",\"Malaysia\",\"Mauritius\",\"Mexico\",\"Morocco\", \"Mozambique\", \"Myanmar (Burma)\",\"Nepal\",\"Netherlands\",\"New Zealand\",\"Nicaragua\",\"Nigeria\",\"Norway\",\"Oman\",\"Pakistan\",\"Panama\",\"Paraguay\",\"Peru\",\"Philippines\",\"Poland\",\"Portugal\",\"Puerto Rico\",\"Romania\",\"Russia\",\"Saudi Arabia\",\"Senegal\",\"Singapore\",\"Slovakia\",\"South Africa\",\"South Korea\",\"Spain\",\"Sri Lanka\",\"Sweden\",\"Switzerland\",\"Taiwan\",\"Tanzania\",\"Thailand\",\"Tunisia\",\"Turkey\",\"Uganda\",\"Ukraine\",\"United Arab Emirates\",\"United Kingdom\",\"United States\",\"Uruguay\",\"Uzbekistan\",\"Venezuela\",\"Vietnam\",\"Zambia\",\"Zimbabwe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc20865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(csv_path, nrows = None):\n",
    "    json_cols = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    df = pd.read_csv(csv_path,\n",
    "                     #converters are dict of functions for converting values in certain columns. Keys can either be integers or column labels.\n",
    "                     #json.loads() method can be used to parse a valid JSON string and convert it into a Python Dictionary.\n",
    "                     #It is mainly used for deserializing native string, byte, or byte array which consists of JSON data into Python Dictionary.\n",
    "                     converters = {col: json.loads for col in json_cols},                                                                         \n",
    "                         dtype = {'fullVisitorId': 'str'}, # Important!!\n",
    "                         nrows = nrows)\n",
    "    for col in json_cols:\n",
    "        # for each column, flatten data frame such that the values of a single col are spread in different cols\n",
    "        # This will use subcol as names of flat_col.columns\n",
    "        flat_col = json_normalize(df[col])\n",
    "        # Name the columns in this flatten data frame as col.subcol for tracability\n",
    "        flat_col.columns = [f\"{col}.{subcol}\" for subcol in flat_col.columns]\n",
    "        # Drop the json_col and instead add the new flat_col\n",
    "        df = df.drop(col, axis = 1).merge(flat_col, right_index = True, left_index = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "csv_test_path = './test_v2.csv'\n",
    "test = load_df(csv_test_path, nrows = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb8fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d79c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify non opearting countries \n",
    "lowCountries=[]\n",
    "hitWithCountry = {}\n",
    "sum = 0\n",
    "cnt = test['geoNetwork.country'].value_counts(sort=True)\n",
    "print(cnt)\n",
    "for i in range(0, cnt.size):\n",
    "    sum = sum + cnt[i]\n",
    "\n",
    "#Identify the Non Operating Countries\n",
    "\n",
    "for i in range(0, cnt.size):\n",
    "    #print(f\"Country Name: {cnt.index[i]} Frequency: {(cnt[i]/sum)*100}%\" )\n",
    "    if((cnt[i]/sum)*100 < 0.01):\n",
    "        lowCountries.append(cnt.index[i]) \n",
    "len(lowCountries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0328bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration for Rule 1\n",
    "#categories = df['Predictions'].to_numpy()\n",
    "colormap = np.array(['g', 'r'])\n",
    "hits1 = {}\n",
    "test['totals.hits'] = pd.to_numeric(test['totals.hits'])\n",
    "for indx in range(test.index.start, test.index.stop):\n",
    "    #print(indx)\n",
    "    country = test.at[indx, 'geoNetwork.country']\n",
    "    oneIndex = (test.at[indx,'hits'])\n",
    "    #hitsInfo = ast.literal_eval(oneIndex)\n",
    "    #print(hitsInfo)\n",
    "    if country in lowCountries:\n",
    "        if country not in hits1.keys():\n",
    "            hits1[country] = (test.at[indx, 'totals.hits'])\n",
    "        elif country in hits1.keys():\n",
    "            hits1[country] = hits1[country] + (test.at[indx, 'totals.hits'])\n",
    "len(hits1)\n",
    "f = plt.figure(figsize=(12, 4))\n",
    "f = plt.scatter(hits1.keys(), hits1.values())\n",
    "f = plt.xlabel('country')\n",
    "f = plt.ylabel('Total Hits')\n",
    "f = plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08004356",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##First Rule : If total hits of a session from NON Opearting/rare countries is outlier means 1.5 IQR from Q3\n",
    "cnt=0\n",
    "sum=0\n",
    "lowCountries=[]\n",
    "hitWithCountry = {}\n",
    "cnt = test['geoNetwork.country'].value_counts(sort=True)\n",
    "\n",
    "for i in range(0, cnt.size):\n",
    "    sum = sum + cnt[i]\n",
    "\n",
    "#Identify the Non Operating Countries\n",
    "\n",
    "for i in range(0, cnt.size):\n",
    "    #print(f\"Country Name: {cnt.index[i]} Frequency: {(cnt[i]/sum)*100}%\" )\n",
    "    if((cnt[i]/sum)*100 < 0.01):\n",
    "        lowCountries.append(cnt.index[i]) \n",
    "    \n",
    "\n",
    "inputCountries = test.at[0,'geoNetwork.country']\n",
    "\n",
    "test['totals.hits'] = pd.to_numeric(test['totals.hits'])\n",
    "data = test['totals.hits']\n",
    "np.median(data)\n",
    "upper_q = np.percentile(data, 75)\n",
    "lower_q = np.percentile(data, 25)\n",
    "iqr = upper_q - lower_q\n",
    "upper_whisker = data[data<=upper_q+1.5*iqr].max()\n",
    "lower_whisker = data[data>=lower_q-1.5*iqr].min()\n",
    "#print(upper_whisker)\n",
    "\n",
    "for indx in range(test.index.start, test.index.stop):\n",
    "    #print(indx)\n",
    "    country = test.at[indx, 'geoNetwork.country']\n",
    "    oneIndex = (test.at[indx,'hits'])\n",
    "    #hitsInfo = ast.literal_eval(oneIndex)\n",
    "    #print(hitsInfo)\n",
    "    if country not in op_countries:\n",
    "        hit = test.at[indx, 'totals.hits']\n",
    "        if int(hit) > upper_whisker:\n",
    "#             print(indx)\n",
    "            test.loc[indx, 'isAnomaly'] = 1\n",
    "            test.loc[indx, 'reason'] = \"total hits of a session from NON Opearting/rare countries is outlier\"\n",
    "#            print(f\"Index Number : {indx} Anomalous traffic Info: {country} Total Hits : {int(hit)} Visit Date:{test.at[indx, 'date']}\")\n",
    "#             for ind in range(0, len(hitsInfo)):\n",
    "#                 print(f\"Hit Number: {hitsInfo[ind]['hitNumber']} Visited Page: {hitsInfo[ind]['appInfo']['screenName']} Keyword: {test.at[indx, 'trafficSource.keyword']}\")\n",
    "        #print(f\"Country Name: {country} --> Total hits: {hit}\")\n",
    "#         if country in hitWithCountry.keys():\n",
    "#             hitWithCountry[country] += 1\n",
    "#         else:\n",
    "#             hitWithCountry[country]=1\n",
    "\n",
    "#newArr = test['hits'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c3fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "oneIndex = (test.at[0,'hits'])\n",
    "result = ast.literal_eval(oneIndex)\n",
    "#print(type(result))\n",
    "for ind in range(0, len(result)):\n",
    "#     temp = ast.literal_eval(oneIndex)\n",
    "#     print(f\"{temp}\")\n",
    "    print(f\"Hit Number: {result[ind]['page']['pageTitle']} Visited Page: {result[ind]['appInfo']['screenName']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2529dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration for Rule 2\n",
    "#categories = df['Predictions'].to_numpy()\n",
    "colormap = np.array(['g', 'r'])\n",
    "views1 = {}\n",
    "test['totals.pageviews'] = pd.to_numeric(test['totals.pageviews'])\n",
    "for indx in range(test.index.start, test.index.stop):\n",
    "    #print(indx)\n",
    "    country = test.at[indx, 'geoNetwork.country']\n",
    "    #oneIndex = (test.at[indx,'totals.pageviews'])\n",
    "    #hitsInfo = ast.literal_eval(oneIndex)\n",
    "    #print(hitsInfo)\n",
    "    if country in lowCountries:\n",
    "        if country not in views1.keys():\n",
    "            views1[country] = (test.at[indx, 'totals.pageviews'])\n",
    "        elif country in views1.keys():\n",
    "            views1[country] = views1[country] + (test.at[indx, 'totals.hits'])\n",
    "len(views1)\n",
    "f = plt.figure(figsize=(12, 4))\n",
    "f = plt.scatter(views1.keys(), views1.values())\n",
    "f = plt.xlabel('country')\n",
    "f = plt.ylabel('Total PageViews')\n",
    "f = plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Rule 2 : if total pageViews from a session is outlier from RareCountries\n",
    "sum = 0\n",
    "test['totals.pageviews'] = pd.to_numeric(test['totals.pageviews'])\n",
    "##Replace NaN Values with Zero\n",
    "test['totals.pageviews'].fillna(value = 0, inplace = True)\n",
    "data = test['totals.pageviews']\n",
    "upper_q = np.percentile(data, 75)\n",
    "lower_q = np.percentile(data, 25)\n",
    "iqr = upper_q - lower_q\n",
    "upper_whisker = data[data<=upper_q+1.5*iqr].max()\n",
    "lower_whisker = data[data>=lower_q-1.5*iqr].min()\n",
    "lower_whisker\n",
    "\n",
    "## Rule based on the Total PageView\n",
    "for ind in range(len(test)):\n",
    "    pageView = test.at[ind , 'totals.pageviews']\n",
    "    country = test.at[ind , 'geoNetwork.country']\n",
    "    if pageView > upper_whisker and country not in op_countries :\n",
    "        print(ind)\n",
    "        test.loc[ind, 'isAnomaly'] = 1\n",
    "        test.loc[ind, 'reason'] = \"total pageViews from a session is outlier from RareCountries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration for Rule 3\n",
    "#categories = df['Predictions'].to_numpy()\n",
    "colormap = np.array(['g', 'r'])\n",
    "\n",
    "f = plt.figure(figsize=(12, 4))\n",
    "f = plt.scatter(test['geoNetwork.country'], test['totals.pageviews'])\n",
    "f = plt.xlabel('country')\n",
    "f = plt.ylabel('Total PageViews')\n",
    "f = plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b24a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Rule 3 : if total pageViews from a session is absurdly higher from any Country\n",
    "sum = 0\n",
    "\n",
    "data = test['totals.pageviews']\n",
    "upper_q = np.percentile(data, 75)\n",
    "lower_q = np.percentile(data, 25)\n",
    "iqr = upper_q - lower_q\n",
    "upper_whisker = data[data<=upper_q+1.5*iqr].max()\n",
    "lower_whisker = data[data>=lower_q-1.5*iqr].min()\n",
    "lower_whisker\n",
    "\n",
    "## Rule based on the Total PageView\n",
    "for i in range(len(test)):\n",
    "    pageView = test.loc[i , 'totals.pageviews']\n",
    "    country = test.loc[i , 'geoNetwork.country']\n",
    "    if pageView > upper_whisker*10:\n",
    "        print(i)\n",
    "        test.loc[i, 'isAnomaly'] = 1\n",
    "        test.loc[i, 'reason'] = \"total pageViews from a session is absurdly higher from any Country\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c57637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test['totals.pageviews']\n",
    "plt.hist(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75173720",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip(test.fullVisitorId.duplicated().index, test.fullVisitorId.duplicated()):\n",
    "    if j == True:\n",
    "        print(test.loc[i, 'fullVisitorId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "test['totals.hits'] = pd.to_numeric(test['totals.hits'])\n",
    "mean = test['totals.hits'].mean()\n",
    "std = test['totals.hits'].std()\n",
    "maximum = test['totals.hits'].max()\n",
    "\n",
    "#plt.boxplot(test['totals.hits'])\n",
    "#plt.xlim(xmin=0, xmax = 501)\n",
    "#plt.show()\n",
    "data = test['totals.hits']\n",
    "np.median(data)\n",
    "upper_q = np.percentile(data, 75)\n",
    "lower_q = np.percentile(data, 25)\n",
    "iqr = upper_q - lower_q\n",
    "upper_whisker = data[data<=upper_q+1.5*iqr].max()\n",
    "lower_whisker = data[data>=lower_q-1.5*iqr].min()\n",
    "upper_whisker\n",
    "data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration for Rule 4\n",
    "#categories = df['Predictions'].to_numpy()\n",
    "colormap = np.array(['g', 'r'])\n",
    "tos1 = {}\n",
    "test['totals.timeOnSite'] = pd.to_numeric(test['totals.timeOnSite'])\n",
    "for indx in range(test.index.start, test.index.stop):\n",
    "    #print(indx)\n",
    "    country = test.at[indx, 'geoNetwork.country']\n",
    "    #oneIndex = (test.at[indx,'totals.pageviews'])\n",
    "    #hitsInfo = ast.literal_eval(oneIndex)\n",
    "    #print(hitsInfo)\n",
    "    if country in lowCountries:\n",
    "        if country not in tos1.keys():\n",
    "            tos1[country] = (test.at[indx, 'totals.pageviews'])\n",
    "        elif country in tos1.keys():\n",
    "            tos1[country] = tos1[country] + (test.at[indx, 'totals.hits'])\n",
    "len(tos1)\n",
    "f = plt.figure(figsize=(12, 4))\n",
    "f = plt.scatter(tos1.keys(), tos1.values())\n",
    "f = plt.xlabel('country')\n",
    "f = plt.ylabel('Session Duration')\n",
    "f = plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule 4: Session duration is outlier for non operating countries\n",
    "## total.timeOnSite means session duration of a session\n",
    "sum = 0\n",
    "test['totals.timeOnSite'] = pd.to_numeric(test['totals.timeOnSite'])\n",
    "test['totals.timeOnSite'].fillna(value = 0, inplace = True)\n",
    "data = test['totals.timeOnSite']\n",
    "upper_q = np.percentile(data, 75)\n",
    "lower_q = np.percentile(data, 25)\n",
    "iqr = upper_q - lower_q\n",
    "upper_whisker = data[data<=upper_q+1.5*iqr].max()\n",
    "lower_whisker = data[data>=lower_q-1.5*iqr].min()\n",
    "for i in range(len(test)):\n",
    "    timeOnSite = test.at[i, 'totals.timeOnSite']\n",
    "    country = test.at[i, 'geoNetwork.country']\n",
    "    if timeOnSite > upper_whisker and country in lowCountries:\n",
    "        print(i)\n",
    "        test.loc[i, 'isAnomaly'] = 1\n",
    "        test.loc[i, 'reason'] = \"Session duration is outlier for non operating countries\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c224906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining how many users come to our system in an hour to create a baseline for rule 5\n",
    "#test['visitStartTime'] = pd.to_datetime(test['visitStartTime'], unit='s')\n",
    "usersWithDate = {}\n",
    "test['date'] = pd.to_numeric(test['date'])\n",
    "test = test.sort_values(by = 'visitStartTime')\n",
    "test = test.reset_index(drop = True)\n",
    "#print(test)\n",
    "timeDiff = test['visitStartTime'].max() - test['visitStartTime'].min()\n",
    "dayDiff = timeDiff / 86400\n",
    "dayDiff = dayDiff.round()\n",
    "print(dayDiff)\n",
    "UsersPerDay = (test['fullVisitorId'].count() / dayDiff).round()\n",
    "UsersPerHour = (UsersPerDay/24).round()\n",
    "hourCount = 86400/24\n",
    "startTime = test.loc[0 , 'visitStartTime']\n",
    "sumOfUsers = 0\n",
    "userCounts=[]\n",
    "for itr in range(len(test)):\n",
    "    #print(test.loc[itr, 'visitStartTime'])\n",
    "    if(test.loc[itr, 'visitStartTime'] > (startTime + 3600)):\n",
    "        startTime = test.loc[itr, 'visitStartTime']\n",
    "        usersWithDate[startTime] = sumOfUsers\n",
    "        userCounts.append(sumOfUsers)\n",
    "        sumOfUsers = 0\n",
    "    sumOfUsers = sumOfUsers+1\n",
    "len(userCounts)\n",
    "#test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f8d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(userCounts)\n",
    "plt.boxplot(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ebaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories = df['Predictions'].to_numpy()\n",
    "colormap = np.array(['g', 'r'])\n",
    "\n",
    "f = plt.figure(figsize=(12, 4))\n",
    "f = plt.scatter(usersWithDate.keys(), usersWithDate.values())\n",
    "f = plt.xlabel('date')\n",
    "f = plt.ylabel('usersHour')\n",
    "f = plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d9f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule 5 : Detect Anomalous users generating higher volume of traffic in an hour\n",
    "sum = 0\n",
    "anomalousHours = []\n",
    "times = {}\n",
    "data = list(usersWithDate.values())\n",
    "upper_q = np.percentile(data, 75)\n",
    "lower_q = np.percentile(data, 25)\n",
    "iqr = upper_q - lower_q\n",
    "upper_whisker = upper_q+1.5*iqr\n",
    "lower_whisker = lower_q-1.5*iqr\n",
    "for i,j in usersWithDate.items():\n",
    "    if j > upper_whisker:\n",
    "        anomalousHours.append(i)\n",
    "\n",
    "for i in range(len(anomalousHours)):\n",
    "    for j in range(len(test)):\n",
    "        endTime = anomalousHours[i]\n",
    "        startTime = endTime - 3600\n",
    "        if test.loc[j, 'visitStartTime'] >=startTime and test.loc[j, 'visitStartTime'] <= endTime:\n",
    "            if j not in times:\n",
    "                times[j] = (test.loc[j, 'totals.hits'])\n",
    "            elif j not in times:\n",
    "                times[j] = times[j] + (test.loc[j, 'totals.hits'])\n",
    "    data = list(times.values())\n",
    "    upper_q = np.percentile(data, 75)\n",
    "    lower_q = np.percentile(data, 25)\n",
    "    iqr = upper_q - lower_q\n",
    "    upper_whisker = upper_q+1.5*iqr\n",
    "    lower_whisker = lower_q-1.5*iqr\n",
    "#     print(upper_whisker)\n",
    "    for a, b in times.items():\n",
    "        if b > upper_whisker*5:\n",
    "            print(a)\n",
    "            test.loc[a, 'isAnomaly'] = 1\n",
    "            test.loc[a, 'reason'] = \"user generating unusual higher volume of traffic in an hour\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule 6: Search Keywords having quotations or absurd file types\n",
    "new = test[test['trafficSource.keyword'].notna()]\n",
    "new.head()\n",
    "new = new[new['trafficSource.keyword'] != '(not set)']\n",
    "new = new[new['trafficSource.keyword'] != '(not provided)']\n",
    "new = new[new['trafficSource.keyword'] != '(automatic matching)']\n",
    "for index in new.index:\n",
    "    if \"\\\"\" in new.loc[index,'trafficSource.keyword'] or \".html\" in new.loc[index,'trafficSource.keyword'] or \"script\" in new.loc[index,'trafficSource.keyword']:\n",
    "        print(index)\n",
    "        test.loc[index, 'isAnomaly'] = 1\n",
    "        test.loc[index, 'reason'] = \"Search Keywords having SQL Injection, XSS Script or file extension types\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16221a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_countries = [\"Algeria\", \"Angola\", \"Argentina\", \"Australia\", \"Austria\", \"Bahrain\", \"Bangladesh\", \"Belarus\", \"Belgium\", \"Brazil\", \"Cambodia\", \"Cameroon\", \"Canada\" , \"Chile\", \"Colombia\", \"Costa Rica\", \"Czechia\", \"Denmark\", \"Dominican Republic\" , \"Ecuador\",\"Egypt\", \"El Salvador\",  \"Ethiopia\", \"Finland\",\"France\", \"Georgia\",\"Germany\",\"Ghana\",\"Greece\", \"Guatemala\",\"Hong Kong\",\"Hungary\", \"India\",\"Indonesia\",\"Ireland\",\"Israel\",\"Italy\",\"Japan\",\"Jordan\",\"Kazakhstan\",\"Kenya\",\"Kuwait\",\"Lebanon\",\"Madagascar\",\"Malaysia\",\"Mauritius\",\"Mexico\",\"Morocco\", \"Mozambique\", \"Myanmar (Burma)\",\"Nepal\",\"Netherlands\",\"New Zealand\",\"Nicaragua\",\"Nigeria\",\"Norway\",\"Oman\",\"Pakistan\",\"Panama\",\"Paraguay\",\"Peru\",\"Philippines\",\"Poland\",\"Portugal\",\"Puerto Rico\",\"Romania\",\"Russia\",\"Saudi Arabia\",\"Senegal\",\"Singapore\",\"Slovakia\",\"South Africa\",\"South Korea\",\"Spain\",\"Sri Lanka\",\"Sweden\",\"Switzerland\",\"Taiwan\",\"Tanzania\",\"Thailand\",\"Tunisia\",\"Turkey\",\"Uganda\",\"Ukraine\",\"United Arab Emirates\",\"United Kingdom\",\"United States\",\"Uruguay\",\"Uzbekistan\",\"Venezuela\",\"Vietnam\",\"Zambia\",\"Zimbabwe\"]\n",
    "\n",
    "op_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule 7: Sentiment Analysis on the searched keywords\n",
    "for index in new.index:\n",
    "    country = new.loc[index, 'geoNetwork.country']\n",
    "    if country not in op_countries:\n",
    "        doc = nlp(new.loc[index,'trafficSource.keyword'])\n",
    "        for i, sentence in enumerate(doc.sentences):\n",
    "            if sentence.sentiment == 0:\n",
    "                print(index)\n",
    "                test.loc[index, 'isAnomaly'] = 1\n",
    "                test.loc[index, 'reason'] = \"Sensitive Product Search or Service Lines\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c89d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rule 8 Attempt to visit employee only resource pages or newsletter registration pages\n",
    "sum = 0;\n",
    "for index in test.index:\n",
    "    country = test.loc[index, 'geoNetwork.country']\n",
    "    if country not in op_countries:\n",
    "        oneIndex = (test.loc[index,'hits'])\n",
    "        result = ast.literal_eval(oneIndex)\n",
    "        #print(type(result))\n",
    "        for ind in range(0, len(result)):\n",
    "            pageString = result[ind]['page']['pagePath']\n",
    "            if \"customerinfo\" in pageString:\n",
    "                print(ind)\n",
    "                test.loc[index, 'isAnomaly'] = 1\n",
    "                test.loc[index, 'reason'] = \"Attempt to visit employee only resource pages or newsletter registration pages from Non Operating Country\"\n",
    "#             sum = sum + 1\n",
    "        #     temp = ast.literal_eval(oneIndex)\n",
    "        #     print(f\"{temp}\")\n",
    "#             print(f\"Hit Number: {result[ind]['hitNumber']} Visited Page: {result[ind]['page']['pagePath']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb18ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in test.index:\n",
    "    print(index)\n",
    "    if pd.isna(test.loc[index,'trafficSource.keyword']):\n",
    "        test.loc[index, 'NLPValue'] = 1\n",
    "    else :\n",
    "        if (test.loc[index,'trafficSource.keyword'] == '(not set)'):\n",
    "            test.loc[index, 'NLPValue'] = 1\n",
    "        elif (test.loc[index,'trafficSource.keyword'] == '(not provided)'):\n",
    "            test.loc[index, 'NLPValue'] = 1\n",
    "        elif (test.loc[index,'trafficSource.keyword'] == '(automatic matching)'):\n",
    "            test.loc[index, 'NLPValue'] = 1\n",
    "        else:\n",
    "            doc = nlp(test.loc[index,'trafficSource.keyword'])\n",
    "            for i, sentence in enumerate(doc.sentences):\n",
    "                test.loc[index, 'NLPValue'] = sentence.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ec41c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('./labelledOut.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36176b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['isAnomaly'].fillna(value = 0, inplace = True)\n",
    "aomalousSessions = test[test['isAnomaly'] == 1]\n",
    "len(aomalousSessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361377dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "aomalousSessions['reason']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f74a4e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "aomalousSessions.to_csv('./out.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
